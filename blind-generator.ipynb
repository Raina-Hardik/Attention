{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Aim\n",
    "\n",
    "The model intends on blindly generating Shakespearean text. An attempt to tackle Attention Architecture(char level)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1943ec7bdf9379a5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Constants"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f421acc847cdbc3"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\hardi\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "dataset = 'combined-works-external'\n",
    "from huggingface_hub import login\n",
    "login(token='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')\n",
    "\n",
    "import torch\n",
    "import torch_directml\n",
    "dml = torch_directml.device()\n",
    "from params import params\n",
    "import Model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T08:09:42.454394700Z",
     "start_time": "2023-09-25T08:09:39.653639200Z"
    }
   },
   "id": "a62fa4d058ec23e9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Explore"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd0736275e55d8f5"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size: 1115394\n",
      "Total Unique Chars: 65\n",
      "All Chars: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "with open(dataset,'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "print(f'Total Size: {len(content)}')\n",
    "\n",
    "unique_elements = sorted(list(set(content)))\n",
    "\n",
    "print(f'Total Unique Chars: {len(unique_elements)}')\n",
    "print(f'All Chars: {\"\".join(unique_elements)}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T08:09:42.511536400Z",
     "start_time": "2023-09-25T08:09:42.456599400Z"
    }
   },
   "id": "915b5c4239bfa41d"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [35, 46, 63, 1, 39, 51, 1, 21, 1, 43, 60, 43, 52, 1, 42, 53, 47, 52, 45, 1, 41, 46, 39, 56, 1, 40, 39, 57, 43, 42, 1, 39, 56, 41, 46, 47, 58, 43, 41, 58, 59, 56, 43, 8, 8, 8]\n",
      "Decoded: Why am I even doing char based architecture...\n"
     ]
    }
   ],
   "source": [
    "# Mappings\n",
    "\n",
    "char_to_int = {ch:i for i,ch in enumerate(unique_elements)}\n",
    "int_to_char = {i:ch for i,ch in enumerate(unique_elements)}\n",
    "\n",
    "# Encoding & Decoding\n",
    "\n",
    "encode = lambda string: [char_to_int[ch] for ch in string]\n",
    "decode = lambda integer: ''.join([int_to_char[i] for i in integer])\n",
    "\n",
    "print(f'Encoded: {encode(\"Why am I even doing char based architecture...\")}')\n",
    "print(f'Decoded: {decode(encode(\"Why am I even doing char based architecture...\"))}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T08:09:42.519546700Z",
     "start_time": "2023-09-25T08:09:42.495225700Z"
    }
   },
   "id": "a1706ab30b570b9"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "privateuseone:0\n",
      "<class 'torch.device'>\n"
     ]
    }
   ],
   "source": [
    "print(dml)\n",
    "print(torch.device)\n",
    "# \n",
    "# from datasets import load_dataset\n",
    "# ds = load_dataset(\"uonlp/CulturaX\",\n",
    "#                   language=\"en\",\n",
    "#                   use_auth_token=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T08:09:42.587955700Z",
     "start_time": "2023-09-25T08:09:42.523555300Z"
    }
   },
   "id": "6c64a302f0993fbe"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(content), dtype=torch.long)\n",
    "print(data.shape, data.dtype)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T08:09:42.680299100Z",
     "start_time": "2023-09-25T08:09:42.547538700Z"
    }
   },
   "id": "bd609cff4dd33150"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "train = data[:n]\n",
    "test = data[n:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T08:09:42.696647800Z",
     "start_time": "2023-09-25T08:09:42.684297900Z"
    }
   },
   "id": "f3fbcfba1b841131"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    loaded_data = train if split == 'train' else test\n",
    "    ix = torch.randint(len(loaded_data) - params[\"context_length\"], (params[\"batch_size\"],))\n",
    "    x = torch.stack([loaded_data[i:i+params[\"context_length\"]] for i in ix])\n",
    "    y = torch.stack([loaded_data[i+1:i+params[\"context_length\"]+1] for i in ix])\n",
    "    x, y = x.to(dml), y.to(dml)\n",
    "    return x, y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T08:09:42.740920300Z",
     "start_time": "2023-09-25T08:09:42.700640400Z"
    }
   },
   "id": "151e9df1a6fa9f37"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(params[\"eval_iters\"])\n",
    "        for k in range(params[\"eval_iters\"]):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T08:09:42.744960100Z",
     "start_time": "2023-09-25T08:09:42.714729Z"
    }
   },
   "id": "37a12cc959517ba5"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x15701449650>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T08:09:42.760968200Z",
     "start_time": "2023-09-25T08:09:42.729235700Z"
    }
   },
   "id": "6fd18357031a7e30"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = Model.BigramLanguageModel(len(unique_elements), dml)\n",
    "m = model.to(dml)\n",
    "\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=params[\"learning_rate\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T08:09:42.912352600Z",
     "start_time": "2023-09-25T08:09:42.760968200Z"
    }
   },
   "id": "6c7de12b2a77b16"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.2751, val loss 4.2735\n",
      "step 100: train loss 2.6605, val loss 2.6649\n",
      "step 200: train loss 2.5008, val loss 2.5093\n",
      "step 300: train loss 2.4245, val loss 2.4248\n",
      "step 400: train loss 2.3631, val loss 2.3588\n",
      "step 500: train loss 2.3139, val loss 2.3289\n",
      "step 600: train loss 2.2697, val loss 2.2807\n",
      "step 700: train loss 2.2165, val loss 2.2263\n",
      "step 800: train loss 2.1748, val loss 2.1995\n",
      "step 900: train loss 2.1458, val loss 2.1776\n",
      "step 1000: train loss 2.1013, val loss 2.1482\n",
      "step 1100: train loss 2.0842, val loss 2.1283\n",
      "step 1200: train loss 2.0572, val loss 2.0982\n",
      "step 1300: train loss 2.0289, val loss 2.0876\n",
      "step 1400: train loss 2.0147, val loss 2.0651\n",
      "step 1500: train loss 1.9892, val loss 2.0508\n",
      "step 1600: train loss 1.9593, val loss 2.0193\n",
      "step 1700: train loss 1.9492, val loss 2.0334\n",
      "step 1800: train loss 1.9186, val loss 1.9947\n",
      "step 1900: train loss 1.9154, val loss 1.9988\n",
      "step 2000: train loss 1.8923, val loss 1.9872\n",
      "step 2100: train loss 1.8749, val loss 1.9822\n",
      "step 2200: train loss 1.8665, val loss 1.9711\n",
      "step 2300: train loss 1.8477, val loss 1.9680\n",
      "step 2400: train loss 1.8334, val loss 1.9487\n",
      "step 2500: train loss 1.8307, val loss 1.9445\n",
      "step 2600: train loss 1.8200, val loss 1.9239\n",
      "step 2700: train loss 1.8023, val loss 1.9220\n",
      "step 2800: train loss 1.7885, val loss 1.9164\n",
      "step 2900: train loss 1.7796, val loss 1.9173\n",
      "step 3000: train loss 1.7722, val loss 1.8989\n",
      "step 3100: train loss 1.7631, val loss 1.9136\n",
      "step 3200: train loss 1.7574, val loss 1.8986\n",
      "step 3300: train loss 1.7443, val loss 1.8940\n",
      "step 3400: train loss 1.7458, val loss 1.8835\n",
      "step 3500: train loss 1.7508, val loss 1.8728\n",
      "step 3600: train loss 1.7279, val loss 1.8836\n",
      "step 3700: train loss 1.7300, val loss 1.8805\n",
      "step 3800: train loss 1.7175, val loss 1.8650\n",
      "step 3900: train loss 1.7179, val loss 1.8592\n",
      "step 4000: train loss 1.7020, val loss 1.8590\n",
      "step 4100: train loss 1.7009, val loss 1.8531\n",
      "step 4200: train loss 1.6933, val loss 1.8461\n",
      "step 4300: train loss 1.6975, val loss 1.8420\n",
      "step 4400: train loss 1.6844, val loss 1.8406\n",
      "step 4500: train loss 1.6667, val loss 1.8473\n",
      "step 4600: train loss 1.6767, val loss 1.8194\n",
      "step 4700: train loss 1.6707, val loss 1.8278\n",
      "step 4800: train loss 1.6699, val loss 1.8294\n",
      "step 4900: train loss 1.6698, val loss 1.8267\n",
      "step 4999: train loss 1.6517, val loss 1.8266\n"
     ]
    }
   ],
   "source": [
    "for i in range(params[\"max_iters\"]):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if i % params[\"eval_interval\"] == 0 or i == params[\"max_iters\"] - 1:\n",
    "        losses = estimate_loss(m)\n",
    "        print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T08:26:43.062932600Z",
     "start_time": "2023-09-25T08:09:42.913359300Z"
    }
   },
   "id": "1312dda833e8ea86"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hardi\\DataspellProjects\\AttentionTransformer\\Model.py:131: UserWarning: The operator 'aten::multinomial' is not currently supported on the DML backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at D:\\a\\_work\\1\\s\\pytorch-directml-plugin\\torch_directml\\csrc\\dml\\dml_cpu_fallback.cpp:17.)\n",
      "  idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GULIDIIUS:\n",
      "More to young of the rest will.\n",
      "\n",
      "CORIOLBON:\n",
      "not, lord, my felsh lost and my felliedss,\n",
      "Let bornough you him hillow, here previll'd\n",
      "Not thou hight.\n",
      "\n",
      "MARCIUS:\n",
      "The dead do calrily not me sad, an ssise am suncrm.\n",
      "\n",
      "MARIANA:\n",
      "Ay, I will Ereling, you word due.\n",
      "\n",
      "CUCIOLIUS:\n",
      "O you, Made nuch wolst the, brother,\n",
      "Nroling look's courp are on't\n",
      "sword trow, Canity, I to send of me will.\n",
      "\n",
      "CORIOLANUS:\n",
      "The sights of say, is out, with him do sigow mded wells,\n",
      "But dissuing againful: let him the cordions courn,\n",
      "The havings son blike to see, his I longt me!\n",
      "\n",
      "ESFird Eine does: if thy me; son,\n",
      "For them cevorve again Richard weeph To mautter dead him to my,\n",
      "To sight, such you? the joy catice we to lied,\n",
      "My earth starking to-popese it up told the cautten living like finder:\n",
      "I'll that his sin: atter, sweight court, hath you well weart of the triuch sway.\n",
      "\n",
      "SAWIford! without blaing over of your drudgs.\n",
      "Master you hartowar, sture of him blooding holds,\n",
      "And to to depify, to live, sopse a my pleep is bold savering my Lords,\n",
      "To like to for you. Henquarred thy\n",
      "in him resish: shall countrys wold to-bech ud will bidgar faith.\n",
      "\n",
      "KING RIClanian! Worllow dlaught shall not with them my brother,\n",
      "From the staile. And wwand in these you crother tward,\n",
      "But thou food genclows here jod sorver's not conters acciden:\n",
      "You what yellge to Keep spent, courn.\n",
      "\n",
      "First, 'Ttward foolt, no all more\n",
      "ay ell.\n",
      "\n",
      "QUEEN Einghair foollow of God spillier,\n",
      "Coriousious you too: are the pering upon they our redom the pose: feloot upon with per!\n",
      "How them say, the boing Richard looke\n",
      "Twixtible in me, wrest he great's!\n",
      "\n",
      "AUTTINGHAM:\n",
      "You faft, all weepin have\n",
      "Where I staid be well,\n",
      "Their it crown, wilt my lord,\n",
      "My so resigeful of vatoom you, greet signor him,\n",
      "Proints, thee endianham he king\n",
      "Fullow you his sitele hath chiefil glinge.\n",
      "Thee nor vilih to my rahther, made the restily are'd the kno to quen not be great flow,\n",
      "There crinched, I follow, it at with the Pricion.\n",
      "\n",
      "ISABELLLA:\n",
      "Loing then trown of but kinst ble mreaturiage,\n",
      "Spea\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=dml)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T08:28:11.188640500Z",
     "start_time": "2023-09-25T08:26:43.087868600Z"
    }
   },
   "id": "45bb79b3b13ff7b8"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('SAVED/SAVED_MODEL.pkl', 'wb') as f:\n",
    "    pickle.dump(m, f)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T08:39:47.491053700Z",
     "start_time": "2023-09-25T08:39:47.382294200Z"
    }
   },
   "id": "c7fb970806278d4"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "with open('SAVED/SAVED_MODEL.pkl', 'rb') as f:\n",
    "    temp = pickle.load(f)\n",
    "\n",
    "equal_weights = all(\n",
    "    torch.equal(param1, param2)\n",
    "    for param1, param2 in zip(model.parameters(), temp.parameters())\n",
    ")\n",
    "\n",
    "print(equal_weights)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T08:39:49.984914900Z",
     "start_time": "2023-09-25T08:39:49.554580900Z"
    }
   },
   "id": "628a807412995971"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
