{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Aim\n",
    "\n",
    "The model intends on blindly generating Shakespearean text. An attempt to tackle Attention Architecture(char level)."
   ],
   "metadata": {
    "collapsed": false,
    "id": "1943ec7bdf9379a5"
   },
   "id": "1943ec7bdf9379a5"
  },
  {
   "cell_type": "code",
   "source": [
    "!wget https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/data/tiny-shakespeare.txt -O combined-works-external\n",
    "!pip install tiktoken"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-MXvddVxhjfW",
    "outputId": "b00b9bcf-054b-48bb-a919-bef914c73759",
    "ExecuteTime": {
     "end_time": "2023-09-28T19:33:06.523455100Z",
     "start_time": "2023-09-28T19:33:04.410922300Z"
    }
   },
   "id": "-MXvddVxhjfW",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\hardi\\.conda\\envs\\attentiontransformer\\lib\\site-packages (0.5.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\hardi\\.conda\\envs\\attentiontransformer\\lib\\site-packages (from tiktoken) (2022.7.9)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\hardi\\.conda\\envs\\attentiontransformer\\lib\\site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hardi\\.conda\\envs\\attentiontransformer\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hardi\\.conda\\envs\\attentiontransformer\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hardi\\.conda\\envs\\attentiontransformer\\lib\\site-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hardi\\.conda\\envs\\attentiontransformer\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Constants"
   ],
   "metadata": {
    "collapsed": false,
    "id": "8f421acc847cdbc3"
   },
   "id": "8f421acc847cdbc3"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='privateuseone', index=0)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = 'combined-works-external'\n",
    "import torch\n",
    "import torch_directml\n",
    "dml = torch_directml.device()\n",
    "# dml = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from params import params\n",
    "import Model\n",
    "\n",
    "dml"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a62fa4d058ec23e9",
    "outputId": "10466a62-6c0b-46cb-ede7-0bccd078a2ce",
    "ExecuteTime": {
     "end_time": "2023-09-28T19:33:09.536598700Z",
     "start_time": "2023-09-28T19:33:06.523455100Z"
    }
   },
   "id": "a62fa4d058ec23e9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Explore"
   ],
   "metadata": {
    "collapsed": false,
    "id": "fd0736275e55d8f5"
   },
   "id": "fd0736275e55d8f5"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size: 1115394\n",
      "Total Unique Elements: 301829\n",
      "Some Elements: [5451, 47317, 512, 10438, 584, 10570, 904, 4726, 11, 6865]\n"
     ]
    }
   ],
   "source": [
    "with open(dataset,'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "print(f'Total Size: {len(content)}')\n",
    "\n",
    "import tiktoken\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Function to encode a string\n",
    "def encode(string):\n",
    "    return encoding.encode(string)\n",
    "\n",
    "# Function to decode a list of tokens\n",
    "def decode(tokens):\n",
    "    text = encoding.decode(tokens)\n",
    "    return text\n",
    "\n",
    "unique_elements = encode(content)\n",
    "\n",
    "print(f'Total Unique Elements: {len(unique_elements)}')\n",
    "print(f'Some Elements: {unique_elements[:10]}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "915b5c4239bfa41d",
    "outputId": "81bbf61b-8bff-405e-e8f1-90417c59f0c1",
    "ExecuteTime": {
     "end_time": "2023-09-28T19:33:10.044982700Z",
     "start_time": "2023-09-28T19:33:09.534598900Z"
    }
   },
   "id": "915b5c4239bfa41d"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# # Mappings\n",
    "#\n",
    "# char_to_int = {ch:i for i,ch in enumerate(unique_elements)}\n",
    "# int_to_char = {i:ch for i,ch in enumerate(unique_elements)}\n",
    "#\n",
    "# # Encoding & Decoding\n",
    "#\n",
    "# encode = lambda string: [char_to_int[ch] for ch in string]\n",
    "# decode = lambda integer: ''.join([int_to_char[i] for i in integer])\n",
    "#\n",
    "# print(f'Encoded: {encode(\"Why am I even doing char based architecture...\")}')\n",
    "# print(f'Decoded: {decode(encode(\"Why am I even doing char based architecture...\"))}')"
   ],
   "metadata": {
    "id": "a1706ab30b570b9",
    "ExecuteTime": {
     "end_time": "2023-09-28T19:33:10.064986500Z",
     "start_time": "2023-09-28T19:33:10.036982800Z"
    }
   },
   "id": "a1706ab30b570b9"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "privateuseone:0\n",
      "<class 'torch.device'>\n"
     ]
    }
   ],
   "source": [
    "print(dml)\n",
    "print(torch.device)\n",
    "#\n",
    "# from datasets import load_dataset\n",
    "# ds = load_dataset(\"uonlp/CulturaX\",\n",
    "#                   language=\"en\",\n",
    "#                   use_auth_token=True)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6c64a302f0993fbe",
    "outputId": "34f240d7-06db-447e-90b7-8dcc874ce5c7",
    "ExecuteTime": {
     "end_time": "2023-09-28T19:33:10.109992900Z",
     "start_time": "2023-09-28T19:33:10.051983600Z"
    }
   },
   "id": "6c64a302f0993fbe"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([301829]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(content), dtype=torch.long)\n",
    "print(data.shape, data.dtype)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bd609cff4dd33150",
    "outputId": "dd5a3eb7-5f39-4219-d975-015eb67b0283",
    "ExecuteTime": {
     "end_time": "2023-09-28T19:33:10.287565800Z",
     "start_time": "2023-09-28T19:33:10.068983400Z"
    }
   },
   "id": "bd609cff4dd33150"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "train = data[:n]\n",
    "test = data[n:]"
   ],
   "metadata": {
    "id": "f3fbcfba1b841131",
    "ExecuteTime": {
     "end_time": "2023-09-28T19:33:10.302846300Z",
     "start_time": "2023-09-28T19:33:10.284150100Z"
    }
   },
   "id": "f3fbcfba1b841131"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    loaded_data = train if split == 'train' else test\n",
    "    ix = torch.randint(len(loaded_data) - params[\"context_length\"], (params[\"batch_size\"],))\n",
    "    x = torch.stack([loaded_data[i:i+params[\"context_length\"]] for i in ix])\n",
    "    y = torch.stack([loaded_data[i+1:i+params[\"context_length\"]+1] for i in ix])\n",
    "    x, y = x.to(dml), y.to(dml)\n",
    "    return x, y"
   ],
   "metadata": {
    "id": "151e9df1a6fa9f37",
    "ExecuteTime": {
     "end_time": "2023-09-28T19:33:10.342929Z",
     "start_time": "2023-09-28T19:33:10.307356300Z"
    }
   },
   "id": "151e9df1a6fa9f37"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(params[\"eval_iters\"])\n",
    "        for k in range(params[\"eval_iters\"]):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ],
   "metadata": {
    "id": "37a12cc959517ba5",
    "ExecuteTime": {
     "end_time": "2023-09-28T19:33:10.343925200Z",
     "start_time": "2023-09-28T19:33:10.317365200Z"
    }
   },
   "id": "37a12cc959517ba5"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x21b536fb9b0>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0000)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6fd18357031a7e30",
    "outputId": "fee4f61b-1cb7-4cab-f628-77635e2ec4db",
    "ExecuteTime": {
     "end_time": "2023-09-28T19:33:10.360960Z",
     "start_time": "2023-09-28T19:33:10.330958200Z"
    }
   },
   "id": "6fd18357031a7e30"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.137285 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = Model.BigramLanguageModel(len(unique_elements), dml)\n",
    "m = model.to(dml)\n",
    "\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=params[\"learning_rate\"])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6c7de12b2a77b16",
    "outputId": "770e1718-d3f6-430e-c38e-545c86479e24",
    "ExecuteTime": {
     "end_time": "2023-09-28T19:33:11.159826500Z",
     "start_time": "2023-09-28T19:33:10.363957300Z"
    }
   },
   "id": "6c7de12b2a77b16"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Could not allocate tensor with 1854588600 bytes. There is not enough GPU video memory available!",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 5\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_iters\u001B[39m\u001B[38;5;124m\"\u001B[39m]):\n\u001B[0;32m      2\u001B[0m \n\u001B[0;32m      3\u001B[0m     \u001B[38;5;66;03m# every once in a while evaluate the loss on train and val sets\u001B[39;00m\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m%\u001B[39m params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meval_interval\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m i \u001B[38;5;241m==\u001B[39m params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_iters\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m----> 5\u001B[0m         losses \u001B[38;5;241m=\u001B[39m \u001B[43mestimate_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mm\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstep \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: train loss \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlosses[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, val loss \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlosses[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;66;03m# sample a batch of data\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\AttentionTransformer\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "Cell \u001B[1;32mIn[9], line 9\u001B[0m, in \u001B[0;36mestimate_loss\u001B[1;34m(model)\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meval_iters\u001B[39m\u001B[38;5;124m\"\u001B[39m]):\n\u001B[0;32m      8\u001B[0m     X, Y \u001B[38;5;241m=\u001B[39m get_batch(split)\n\u001B[1;32m----> 9\u001B[0m     logits, loss \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m     losses[k] \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m     11\u001B[0m out[split] \u001B[38;5;241m=\u001B[39m losses\u001B[38;5;241m.\u001B[39mmean()\n",
      "File \u001B[1;32m~\\.conda\\envs\\AttentionTransformer\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\DataspellProjects\\AttentionTransformer\\Model.py:115\u001B[0m, in \u001B[0;36mBigramLanguageModel.forward\u001B[1;34m(self, idx, targets)\u001B[0m\n\u001B[0;32m    113\u001B[0m     logits \u001B[38;5;241m=\u001B[39m logits\u001B[38;5;241m.\u001B[39mview(B \u001B[38;5;241m*\u001B[39m T, C)\n\u001B[0;32m    114\u001B[0m     targets \u001B[38;5;241m=\u001B[39m targets\u001B[38;5;241m.\u001B[39mview(B \u001B[38;5;241m*\u001B[39m T)\n\u001B[1;32m--> 115\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlogits\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtargets\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    117\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m logits, loss\n",
      "File \u001B[1;32m~\\.conda\\envs\\AttentionTransformer\\lib\\site-packages\\torch\\nn\\functional.py:3029\u001B[0m, in \u001B[0;36mcross_entropy\u001B[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001B[0m\n\u001B[0;32m   3027\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3028\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[1;32m-> 3029\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_Reduction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_enum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Could not allocate tensor with 1854588600 bytes. There is not enough GPU video memory available!"
     ]
    }
   ],
   "source": [
    "for i in range(params[\"max_iters\"]):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if i % params[\"eval_interval\"] == 0 or i == params[\"max_iters\"] - 1:\n",
    "        losses = estimate_loss(m)\n",
    "        print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1312dda833e8ea86",
    "outputId": "aacf7979-ef32-4f41-babc-55898b8ea3bc",
    "ExecuteTime": {
     "end_time": "2023-09-28T19:33:19.932413700Z",
     "start_time": "2023-09-28T19:33:11.164826900Z"
    }
   },
   "id": "1312dda833e8ea86"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "context = torch.tensor(unique_elements[:100], dtype=torch.long, device = dml).view(1, -1)\n",
    "print(context)\n",
    "print(decode(m.generate(context, max_new_tokens=2500)[0].tolist()))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "45bb79b3b13ff7b8",
    "outputId": "9821f386-e057-4c14-df22-cdd131f37e02",
    "ExecuteTime": {
     "start_time": "2023-09-28T19:33:19.929881500Z"
    }
   },
   "id": "45bb79b3b13ff7b8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('SAVED/SAVED_MODEL.pkl', 'wb') as f:\n",
    "    pickle.dump(m, f)\n"
   ],
   "metadata": {
    "id": "c7fb970806278d4",
    "ExecuteTime": {
     "end_time": "2023-09-28T19:33:19.936456800Z",
     "start_time": "2023-09-28T19:33:19.934411200Z"
    }
   },
   "id": "c7fb970806278d4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('SAVED/SAVED_MODEL.pkl', 'rb') as f:\n",
    "    temp = pickle.load(f)\n",
    "\n",
    "equal_weights = all(\n",
    "    torch.equal(param1, param2)\n",
    "    for param1, param2 in zip(m.parameters(), temp.parameters())\n",
    ")\n",
    "\n",
    "print(equal_weights)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "628a807412995971",
    "outputId": "8d4f0f29-3cac-4abf-c981-113dceeb2fae",
    "ExecuteTime": {
     "end_time": "2023-09-28T19:33:19.939409300Z",
     "start_time": "2023-09-28T19:33:19.937412400Z"
    }
   },
   "id": "628a807412995971"
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "UV7dEbrVu5V7",
    "ExecuteTime": {
     "start_time": "2023-09-28T19:33:19.940410200Z"
    }
   },
   "id": "UV7dEbrVu5V7",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jbbvrpz4u55F",
    "outputId": "ec152d54-7652-4772-eaba-10859ca34251",
    "ExecuteTime": {
     "end_time": "2023-09-28T19:33:19.945444500Z",
     "start_time": "2023-09-28T19:33:19.943446100Z"
    }
   },
   "id": "Jbbvrpz4u55F",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
